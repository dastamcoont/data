#xgboost공부
#공부한사이트 : http://jetzt.tistory.com/1041
#2018.03.09

decision tree 의사결정나무
중요도에 따라 분류요인을 정하고 분기를 친다.
균등하게 노드를 분포시켜야 제 성능을 발휘한다. 
Gini 지수이용하여 균등한 트리를 만든다.
장점 : 이해하기 쉽다.
단점 : 입력 데이터의 작은 변동에도 tree구성이 크게 달라진다.
      같은 분류의 데이터가 모여있지 않고 흩어져 있다면 성능에 약점이있다.
이에 따른 대안으로 Boosting 기법이 나왔다.

Boosted Decision Tree
(성능이)약한 학습기를 여러개 사용해서 하나의 강건한 학습결과를 산출

AdaBoost
이전 약한 학습기 결과의 오차를 다른 약한 학습기의 입력에 반영
가중치를 1/n 으로 동일하게 준 이후 약한 학습기의 입력중에 가중치를 반영한다.
장점 : Decision Tree에 비해 성능이 개선되었다.
단점 : 높은 weight를 가진 data point가 존재하면 성능이 크게 떨어진다.

Gradient Boost
보정함수를 추가해서 새로운 트리를 생성한다.
단순한 지수곱셈이 아닌 경사하강법을 사용해서 오류를 최소화한다.
장점 : AdaBoost 보다 성능이 개선되었다.
단점 : 이 방법도 지역최소값에 빠질 수 있다.(개선중)

Gradient Boost 개선
subsampling 
하나의 트리를 학습할 때 모든 학습데이터를 사용하는 것이 아닌
학습데이터 중에 랜덤하게 추출한 데이터를 사용한다.
과적합을 방지한다.

min child weight 
하나의 트리에서 말단 노드가 가져야할 데이터수의 최소값을 제한한다.
장점 : 특이값 때문에 발생하는 변동폭을 줄여 성능을 향상시킨다.


XGBoost
Decision Tree를 구성할 때 병렬 처리기법을 사용한다.
장점 : Gradient Boost에 비해 수행시간, 연산시간이 개선되었다.
단점 : 노드 단위로 먼저 loop를 돌려 병렬처리에 불균형이 발생한다.
FPGBT - feature단위로 먼저 loop를 돌려 작업량을 균등하게 한다.

